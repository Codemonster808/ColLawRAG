# Modelos Recomendados para RAG - Eficiencia y Rendimiento

Esta gu√≠a te ayudar√° a elegir modelos m√°s eficientes y r√°pidos para tu sistema RAG en espa√±ol.

## Problema Actual

El modelo por defecto `meta-llama/llama-3.3-70b-instruct` es muy grande (70B par√°metros) y puede:
- Ser muy lento (30-60+ segundos por consulta)
- No estar disponible en el router de Hugging Face
- Consumir muchos recursos
- Causar timeouts

## Modelos Recomendados para Generaci√≥n (Respuestas)

### üöÄ Opci√≥n 1: Mistral 7B (Recomendado - Balance Perfecto)

**Modelo**: `mistralai/Mistral-7B-Instruct-v0.3`

**Ventajas**:
- ‚úÖ Muy r√°pido (5-15 segundos)
- ‚úÖ Excelente en espa√±ol
- ‚úÖ Optimizado para instrucciones
- ‚úÖ Disponible en router.huggingface.co
- ‚úÖ Gratis con API key

**Configuraci√≥n**:
```bash
# En .env.local o Vercel
HF_GENERATION_MODEL=mistralai/Mistral-7B-Instruct-v0.3
```

---

### ‚ö° Opci√≥n 2: Llama 3.1 8B (Muy R√°pido)

**Modelo**: `meta-llama/Llama-3.1-8B-Instruct`

**Ventajas**:
- ‚úÖ Muy r√°pido (3-10 segundos)
- ‚úÖ Buen rendimiento en espa√±ol
- ‚úÖ Modelo reciente y optimizado
- ‚úÖ Disponible en router.huggingface.co

**Configuraci√≥n**:
```bash
HF_GENERATION_MODEL=meta-llama/Llama-3.1-8B-Instruct
```

---

### üéØ Opci√≥n 3: Qwen 2.5 (Excelente para Espa√±ol)

**Modelo**: `Qwen/Qwen2.5-7B-Instruct`

**Ventajas**:
- ‚úÖ R√°pido (5-12 segundos)
- ‚úÖ Excelente en espa√±ol y multiling√ºe
- ‚úÖ Buen rendimiento en tareas legales
- ‚úÖ Disponible en router.huggingface.co

**Configuraci√≥n**:
```bash
HF_GENERATION_MODEL=Qwen/Qwen2.5-7B-Instruct
```

---

### üåü Opci√≥n 4: Llama 3.2 3B (Ultra R√°pido)

**Modelo**: `meta-llama/Llama-3.2-3B-Instruct`

**Ventajas**:
- ‚úÖ Ultra r√°pido (2-8 segundos)
- ‚úÖ Muy ligero
- ‚úÖ Buen rendimiento para consultas simples
- ‚ö†Ô∏è Menor calidad que modelos m√°s grandes

**Configuraci√≥n**:
```bash
HF_GENERATION_MODEL=meta-llama/Llama-3.2-3B-Instruct
```

---

## Modelos Recomendados para Embeddings (B√∫squeda)

El modelo actual `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` es excelente, pero si necesitas m√°s velocidad:

### Opci√≥n R√°pida: MiniLM

**Modelo**: `sentence-transformers/all-MiniLM-L6-v2`

**Ventajas**:
- ‚úÖ Muy r√°pido
- ‚úÖ Buena calidad
- ‚ö†Ô∏è Menor precisi√≥n que mpnet

**Configuraci√≥n**:
```bash
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

---

## Comparaci√≥n de Modelos

| Modelo | Tama√±o | Velocidad | Calidad Espa√±ol | Recomendado Para |
|--------|--------|-----------|-----------------|------------------|
| **Mistral-7B-Instruct** | 7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Producci√≥n** |
| Llama-3.1-8B-Instruct | 8B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Producci√≥n |
| Qwen2.5-7B-Instruct | 7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Producci√≥n |
| Llama-3.2-3B-Instruct | 3B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Desarrollo/Testing |
| Llama-3.3-70B-Instruct | 70B | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | No recomendado (muy lento) |

---

## C√≥mo Cambiar el Modelo

### Opci√≥n 1: Variable de Entorno Local

1. Edita `.env.local` (o cr√©alo si no existe):
```bash
cd /home/lesaint/Documentos/Cursor/ColLawRAG
nano .env.local
```

2. Agrega o modifica:
```env
HUGGINGFACE_API_KEY=tu_api_key_aqui
HF_GENERATION_MODEL=mistralai/Mistral-7B-Instruct-v0.3
```

3. Reinicia el servidor:
```bash
# Det√©n el servidor (Ctrl+C) y reinicia
npm run dev
```

### Opci√≥n 2: Variable de Entorno en Vercel

1. Ve a Vercel Dashboard ‚Üí Tu Proyecto
2. Settings ‚Üí Environment Variables
3. Agrega o modifica `HF_GENERATION_MODEL`:
   - **Name**: `HF_GENERATION_MODEL`
   - **Value**: `mistralai/Mistral-7B-Instruct-v0.3`
   - **Environment**: Production, Preview
4. Haz un nuevo deploy

---

## Verificar que el Modelo Funciona

### Test R√°pido

```bash
curl -X POST http://localhost:3000/api/rag \
  -H "Content-Type: application/json" \
  -H "Origin: http://localhost:3000" \
  -d '{"query": "¬øQu√© es la acci√≥n de tutela?", "locale": "es"}' \
  -w "\nTiempo: %{time_total}s\n"
```

**Resultado esperado**:
- ‚úÖ Respuesta en menos de 15 segundos
- ‚úÖ Respuesta con contenido relevante
- ‚úÖ Sin errores de timeout

---

## Troubleshooting

### Error: "Model not found" o "Model unavailable"

**Causa**: El modelo no est√° disponible en router.huggingface.co

**Soluci√≥n**: 
1. Verifica que el nombre del modelo sea correcto
2. Prueba con otro modelo de la lista
3. Verifica en https://huggingface.co/models que el modelo existe

### Error: "Timeout" o "Request timeout"

**Causa**: El modelo es muy lento o el timeout es muy corto

**Soluci√≥n**:
1. Cambia a un modelo m√°s r√°pido (Mistral 7B o Llama 3.1 8B)
2. Aumenta el timeout:
```bash
HF_API_TIMEOUT_MS=60000  # 60 segundos
PIPELINE_TIMEOUT_MS=90000  # 90 segundos
```

### Error: "No response" o respuesta vac√≠a

**Causa**: El modelo no est√° generando respuesta o hay un error en el prompt

**Soluci√≥n**:
1. Verifica los logs del servidor
2. Prueba con una consulta m√°s simple
3. Verifica que `HUGGINGFACE_API_KEY` es v√°lida

---

## Recomendaci√≥n Final

**Para producci√≥n**: Usa `mistralai/Mistral-7B-Instruct-v0.3`
- Balance perfecto entre velocidad y calidad
- Excelente en espa√±ol
- Muy estable y confiable

**Para desarrollo/testing**: Usa `meta-llama/Llama-3.2-3B-Instruct`
- Ultra r√°pido
- Suficiente para pruebas

---

## Pr√≥ximos Pasos

1. **Cambia el modelo** a Mistral 7B o Llama 3.1 8B
2. **Reinicia el servidor**
3. **Prueba una consulta** simple
4. **Verifica los tiempos** de respuesta
5. **Ajusta timeouts** si es necesario

---

**√öltima actualizaci√≥n**: 2024-01-15
