proyecto: ColLawRAG
documento_base: docs/mejoras/FASE_5_EVALUACION_Y_OBSERVABILIDAD.md
generado: 2026-02-23
modo: autonomo
prioridad: Media
impacto_estimado: "Habilita medición iterativa; no mejora accuracy directo"
esfuerzo: "3-5 días"
dependencias: ninguna estricta; paralelo a otras fases

avance_2026_02{estado,tareas_hechas,pending,siguiente_payload_para_trabajo}:
  parcial,"5.5 (OpenClaw 2026-02-24)","5.1 5.2 5.3 5.4 (Cursor)","OpenClaw 5.5 completada. Cursor pendiente: 5.1-5.4. Scripts diagnóstico listos."

autonomo{modo,ruta_proyecto,no_modificar,prerequisitos,siguiente_payload,al_fallar}:
  autonomo,ColLawRAG,"docs/mejoras/*.md","data/benchmarks/qa-abogados.json existe; scripts/evaluate-accuracy.mjs existe; npm install",ninguno,"reportar_y_continuar"

multi_agente{habilitado,herramientas,sprint_id,paralelo_en_fase,post_fase_agente,token_budget}:
  true,"cursor,open_claw",Sprint_4_observabilidad,true,cursor,"cursor=intensivo open_claw=ligero"

asignacion_agente[5]{tarea_id,agente,archivos_exclusivos,intensidad_tokens}:
  5.1,cursor,"data/benchmarks/qa-abogados.json scripts/annotate-retrieval-ground-truth.mjs",alta
  5.2,cursor,scripts/evaluate-retrieval.mjs,alta
  5.3,cursor,"lib/tracing.ts lib/rag.ts",alta
  5.4,cursor,scripts/evaluate-accuracy.mjs,alta
  5.5,open_claw,"scripts/full-diagnostic.mjs scripts/compare-diagnostics.mjs",baja

grupos_paralelos[2]{ola,tareas,agentes,nota}:
  1,"5.1 5.2 5.3 5.4",cursor,"Cursor: dataset ground truth, evaluate-retrieval, tracing, juez evaluate-accuracy"
  2,5.5,open_claw,"OpenClaw: solo scripts full-diagnostic y compare-diagnostics (scripts cortos)"

instruccion_herramienta[2]{herramienta,instruccion}:
  cursor,"Ejecuta tareas 5.1 5.2 5.3 5.4 (dataset chunks_esperados, evaluate-retrieval.mjs, lib/tracing.ts + rag.ts, juez en evaluate-accuracy.mjs). No toques full-diagnostic ni compare-diagnostics. Ejecuta post_fase_comando (evaluate-retrieval --limit 10) si aplica."
  open_claw,"Solo tarea 5.5 (ligera): CREAR scripts/full-diagnostic.mjs y scripts/compare-diagnostics.mjs (scripts acotados). No toques dataset evaluate-retrieval tracing rag evaluate-accuracy. No ejecutes post_fase_comando."

cuellos_botella[1]{num,descripcion,archivo}:
  10,Evaluación ciega sin métricas por capa; juez 7B trunca 250 chars,scripts/evaluate-accuracy.mjs data/benchmarks/qa-abogados.json

tareas[5]{id,titulo,archivos_afectados,validacion,instruccion_concreta,comando_validacion}:
  5.1,Ground truth retrieval en dataset,"data/benchmarks/qa-abogados.json scripts/annotate-retrieval-ground-truth.mjs",50 casos con chunks_esperados; 1 crítico por caso,"Añadir campo chunks_esperados a 50 casos en qa-abogados.json (array chunk ids); CREAR scripts/annotate-retrieval-ground-truth.mjs si ayuda. 1 chunk crítico por caso.","node -e \"const d=require('./data/benchmarks/qa-abogados.json'); console.log(d.questions?.filter(q=>q.chunks_esperados).length)\""
  5.2,Métricas retrieval Recall Precision MRR NDCG,scripts/evaluate-retrieval.mjs,Script corre sobre dataset anotado; Recall@10 mayor Recall@5,"CREAR scripts/evaluate-retrieval.mjs: cargar dataset con chunks_esperados; ejecutar retrieval por pregunta; calcular Recall@K Precision@K MRR NDCG@K.","test -f scripts/evaluate-retrieval.mjs; node scripts/evaluate-retrieval.mjs --help 2>&1"
  5.3,Tracing pipeline por request,lib/tracing.ts lib/rag.ts,Trace por paso; GET /api/debug/trace requestId,"CREAR lib/tracing.ts con PipelineTrace y TraceStep. En lib/rag.ts emitir trace por paso (embedding retrieval bm25 reranking generation). Endpoint GET /api/debug/trace?requestId= último 100 en dev.","grep -l TraceStep lib/tracing.ts lib/rag.ts"
  5.4,Mejorar juez LLM no truncar modelo mayor,scripts/evaluate-accuracy.mjs,Respuestas 1500 chars juez; modelo 14B o 72B; max_tokens 500,"En scripts/evaluate-accuracy.mjs: juez con modelo mayor 14B; no truncar respuesta a 250 chars; permitir 1500 chars; criterio relevancia_contexto; max_tokens 500.","grep -E 'max_tokens|relevancia_contexto|1500' scripts/evaluate-accuracy.mjs"
  5.5,Dashboard diagnóstico por capa,"scripts/full-diagnostic.mjs scripts/compare-diagnostics.mjs",Diagnóstico identifica cuello botella; A/B dos runs,"CREAR scripts/full-diagnostic.mjs: ejecutar pipeline por capa y guardar diagnostic-{fecha}.json. CREAR scripts/compare-diagnostics.mjs: comparar dos JSON y mostrar diff.","test -f scripts/full-diagnostic.mjs; test -f scripts/compare-diagnostics.mjs"

archivos_modificar[7]{archivo,accion}:
  data/benchmarks/qa-abogados.json,MODIFICAR agregar chunks_esperados
  scripts/evaluate-retrieval.mjs,CREAR métricas retrieval
  scripts/full-diagnostic.mjs,CREAR diagnóstico por capa
  scripts/compare-diagnostics.mjs,CREAR comparación A/B
  lib/tracing.ts,CREAR PipelineTrace TraceStep
  lib/rag.ts,MODIFICAR emitir traces
  scripts/evaluate-accuracy.mjs,MODIFICAR juez no truncar criterio retrieval

orden_ejecucion[5]{paso,tarea_id,nota}:
  1,5.1,Anotar 50 casos chunks esperados
  2,5.2,Recall@K Precision@K MRR NDCG@K
  3,5.3,TraceStep embedding retrieval bm25 reranking generation
  4,5.4,Juez más capaz que modelo generación
  5,5.5,diagnostic-{fecha}.json

criterio_exito[6]{item}:
  Dataset mayor 50 casos con ground truth retrieval
  Métricas retrieval Recall Precision MRR NDCG implementadas
  Tracing reconstruye request paso a paso
  Juez mayor 14B; no trunca; criterio relevancia_contexto
  Diagnóstico identifica cuello botella correcto
  Comparación A/B funcional

post_fase_comando[1]{orden,comando,descripcion}:
  1,"cd ColLawRAG && node scripts/evaluate-retrieval.mjs --limit 10 2>&1","Ejecutar evaluación retrieval si dataset anotado existe"
